---
title: "Tokenization e stemming em l√≠ngua portuguesa de discursos presidenciais."

categories: [NLP, text mining]

# MUDE APENAS ANO DIA E MES PARA O DIA QUE VOCE NOS ENVIOU
date: '2021-06-25T00:00:00Z' 

draft: no

featured: no

gallery_item: null

image:
  caption: 
  focal_point: 
  preview_only: 
  
  projects: []

subtitle: null

summary: null

# DIGITE NA LISTA ABAIXO OS TRACKS DO SEU CODIGO
tags: 
  - Open Data
  - R
  - NLP
  - Text Mining
  
# DIGITE NA LISTA ABAIXO O NOME DE TODOS OS AUTORES SEM ESPACOS
authors:
  - LucasMussoi
  - MarceloPerlin
  - MarcosHenriqueReichert


---


<script type="text/javascript" src="//cdn.plu.mx/widget-popup.js"></script>


<a href="https://plu.mx/plum/a/?doi=10.17632%2Fk5myphbwwj.1" data-popup="right" data-size="large" class="plumx-plum-print-popup" data-site="plum" data-hide-when-empty="true">Tokenization e stemming em lÌngua portuguesa de discursos presidenciais published at the &amp;quot;Open Code Community&amp;quot;</a>

As premissas que constituem a vasta gama de modelos de *Machine
Learning* e de *Deep Learning* foram trazidas para o ramo da lingu√≠stica
com o intuito de interpretar escritas e falas humanas, **Processamento
de Linguagem Natural** sendo *NLP* seu acr√¥nimo em ingl√™s. Por mais que
tenhamos pouco conhecimento sobre as min√∫cias de uma linguagem, h√°
padr√µes que s√£o identific√°veis nas palavras - afixo, raiz, radical,
etc. Esses elementos constituem a morfologia, suas caracter√≠sticas est√£o
profundamente atreladas aos passos necess√°rios para o pr√©-processamento
de uma base de dados textual: tokeniza√ß√£o, remo√ß√£o de *stop words* e
*inclusive stemming*. A acur√°cia dos modelos est√° diretamente ligada a
qualidade desses passos iniciais portanto, √© de sumo interesse que
tenhamos um vislumbre dessa etapa. Vamos ver um pouco desses processos e
algumas de suas funcionalidades, as quais podem ser diretamente
utilizadas em pesquisas ou apenas com intuito ilustrativo.

# Pr√©-processamanto textual.

## Dados n√£o processados

Como base textual, optamos pelo uso de discursos presidenciais pelo
simples fato de conterem diversos prefixos de tratamento, linguagem
culta (assim esperamos), par√°grafos bem estruturados, n√£o conterem
(assim esperamos, novamente) erros de digita√ß√£o e por serem de livre e
f√°cil acesso
(<https://www.gov.br/planalto/pt-br/acompanhe-o-planalto/discursos/2021>).

    library(tibble) # Biblioteca necess√°ria para o trabalho com dados

    jb_speech <- read.delim("dp_jb.txt", #localiza√ß√£o de arquivo com os discursos
                            header = FALSE, #gera coluna de dados com nome gen√©rico
                            col.names = "txt") #converte nome gen√©rico para txt
    df_speech <- tibble(jb_speech) #transforma na estrutura de dados tibble

## Tokeniza√ß√£o

Utilizamos a fun√ß√£o *unnest\_tokens()* da biblioteca *tidytext* para
realizar a tokeniza√ß√£o dos discursos presidenciais selecionados.

    library(tidytext) #biblioteca para textmining
    library(dplyr) #biblioteca que permite manipula√ß√£o de dataframes

    ## 
    ## Attaching package: 'dplyr'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     filter, lag

    ## The following objects are masked from 'package:base':
    ## 
    ##     intersect, setdiff, setequal, union

    tokens <- df_speech %>% #piping: acessa os discursos
      unnest_tokens(palavras, txt) #tokeniza√ß√£o propriamente dita dos discursos, chama a coluna dos tokens de palavras. txt √© a vari√°vel com os discursos.

    tokens

    ## # A tibble: 7,144 x 1
    ##    palavras
    ##    <chr>   
    ##  1 discurso
    ##  2 15062021
    ##  3 bom     
    ##  4 dia     
    ##  5 n√≥s     
    ##  6 sabemos 
    ##  7 que     
    ##  8 uma     
    ##  9 boa     
    ## 10 imagem  
    ## # ‚Ä¶ with 7,134 more rows

*unnest\_tokens()* converte automaticamente as letras mai√∫sculas em
min√∫sculas de maneira a facilitar a compara√ß√£o com outras databases.
Pode-se ent√£o usar *to\_lower = FALSE* nos argumentos da fun√ß√£o. Manter
as palavras com letras mai√∫sculas serve como proxy para identifica√ß√£o de
nomes pessoais, empresas, entidades governamentais e acr√¥nimos.

Pode-se utilizar o argumento *token = ‚Äúsentences‚Äù* para diferenciar
palavras com letras mai√∫sculas que iniciam frases.

    sentences <- df_speech %>%
      unnest_tokens(sentences, txt, token = "sentences")

    head(sentences)

    ## # A tibble: 6 x 1
    ##   sentences                                                                     
    ##   <chr>                                                                         
    ## 1 discurso - 15062021                                                           
    ## 2 bom dia, n√≥s sabemos que uma boa imagem vale muito mais que um milh√£o de pala‚Ä¶
    ## 3 eu quero come√ßar saudando o nosso minist√©rio das rela√ß√µes exteriores, tendo e‚Ä¶
    ## 4 parab√©ns a todos os servidores do nosso itamaraty na pessoa do nosso ministro‚Ä¶
    ## 5 eu me lembro, senhores embaixadores.                                          
    ## 6 em 1963, eu tinha 8 anos de idade, estava em jundia√≠ indo para campinas, e eu‚Ä¶

    # Usando token = "tweets, √© poss√≠vel fazer a tokeniza√ß√£o por palavras e preservando os USERNAMES, HASHTAGS e URL's de twitters. 

## *Stop words*

*Stop words* por defini√ß√£o carregam muito poucas informa√ß√µes. Nas
l√≠nguas rom√¢nticas, inclusive, s√£o as palavras mais comuns - um
threshold √© estabalecido dentro de um database de textos para que a
palavra seja considerada uma *stop word* - ou palavras funcionais tal
como *a*, *o*, *em*, *no* e suas varia√ß√µes. L√≠nguas com casos como
Russo, Latim, Island√™s a abordagem √© feita de maneira diferente.

Vamos observar a composi√ß√£o prim√°ria desse discuros presidencial
analisando a incid√™ncia das palavras. O intuito √© reduzir a carga de
informa√ß√µes a serem processadas, ajudando a manter um tempo
computacional aceit√°vel e despend√™-lo em fun√ß√µes mais cruciais.

    tokens_rank <- tokens %>% #acessa os tokens
      count(palavras, sort = TRUE) #assimila a cada token sua incid√™ncia

    head(tokens_rank)

    ## # A tibble: 6 x 2
    ##   palavras     n
    ##   <chr>    <int>
    ## 1 que        229
    ## 2 de         228
    ## 3 o          224
    ## 4 a          212
    ## 5 e          193
    ## 6 para       111

√â necess√°rio fazer o download das *stop words* em portugu√™s brasileiro.
H√° diversas fontes, a grande maioria com as mesmas palavras. Realizamos,
portanto, e a elimina√ß√£o dessas palavras baseadas na tokeniza√ß√£o
previamente realizada.

    library(stringr) # Usamos a biblioteca stringr para retirar qualquer espa√ßo em branco do databse de stopwords. 

    ## 
    ## Attaching package: 'stringr'

    ## The following object is masked _by_ '.GlobalEnv':
    ## 
    ##     sentences

    stopwords <- read.delim(
        file = "http://www.labape.com.br/rprimi/ds/stopwords.txt", 
        header = FALSE,
        col.names = "palavras")

    # Com o argumento pattern = " " dizemos que vamos substituir um espa√ßo em branco pelo argumento repl="" (nenhum espa√ßo)
    stopwords <- str_replace_all(string=stopwords$palavras, pattern=" ", repl="")

    head(stopwords)

    ## [1] "de"  "a"   "o"   "que" "e"   "do"

    #Filtramos as palvras que n√£o est√£o na lista de stopwords com o prefixo ! - nega√ß√£o - e as contamos
    clean_speech <- tokens %>%
      filter(!palavras %in% stopwords)

    freq_word_speech <- clean_speech %>%
      count(palavras, sort = TRUE)

    # Exibimos um wordcloud das 50 palavras mais frequentes
    library(wordcloud)

    ## Loading required package: RColorBrewer

    wordcloud(words = freq_word_speech$palavras,
              freq = freq_word_speech$n,
              min.freq = 1,
              max.words = 50,
              random.order = FALSE,
              colors=brewer.pal(8, "Dark2"))

![](Discursos_JB_NLP_preprocessamento_files/figure-markdown_strict/unnamed-chunk-5-1.png)

O contexto √© extremamente importante em *text mining* ent√£o devemos
garantir que as stop words est√£o dentro do universo o qual desejamos
estudar. Atrav√©s das palavras observadas podemos assumir que h√° v√≠cios
de linguagem por parte do presidente. Como as palavras *‚Äúa√≠‚Äù*, *‚Äúcoisa‚Äù*
que n√£o carregam significados reais nenhum no entanto, aparecem com
frequ√™ncia. Para, especificamente, a incid√™ncia das palavras nos
discursos presidenciais de Jair Bolsonaro podemos incluir essas palavras
na nossa lista de *stopwords* (se esse for nosso desejo e n√£o implicar
em nehum problema para o estudo realizado).

## Stemming

Stemming √© uma das maneiras de reduzir a dispers√£o dos dados - o que
pode ser interessante para treinarmos alguns modelo - reduzindo uma
palavra √† sua raiz. No entanto, somos penalizados por estarmos jogando
informa√ß√µes fora.

Para isso, precisamos realizar o download do pacote *rslp* -
desenvolvido por Viviane Moreira Orengo e Christian Huyck do Instituto
de Inform√°tica da Universidade Federal do Rio Grande do Sul (*A Stemming
Algorithmm for the Portuguese Language*).

    devtools::install_github("dfalbel/rslp")

    ## Skipping install of 'rslp' from a github remote, the SHA1 (b8cd6715) has not changed since last install.
    ##   Use `force = TRUE` to force installation

    library(rslp)

    speech_stemm <- tibble(palavras = rslp(clean_speech$palavras))

    freq_word_stemm <- speech_stemm %>%
      count(palavras, sort = TRUE)

    wordcloud(words = freq_word_stemm$palavras,
              freq = freq_word_stemm$n,
              min.freq = 1,
              max.words = 50,
              random.order = FALSE,
              colors=brewer.pal(8, "Dark2"))

![](Discursos_JB_NLP_preprocessamento_files/figure-markdown_strict/unnamed-chunk-6-1.png)

Podemos ver que ap√≥s o stemming, do mesmo database, temos diferen√ßa nas
incid√™ncias de palavras. Palavras distintas podem ter a mesma ra√≠z e
isso se reflete visualmente atrav√©s dos *wordclouds* os quais s√£o
expressivamente distintos.

# Conclus√£o

*Text mining* gradativamente se consolidou como uma importante
ferramenta para as mais diversas √°reas do conhecimento. H√° uma vasta
literatura que aborda n√£o somente o uso, mas os algoritmos por tr√°s das
fun√ß√µes aqui utlizadas. A import√¢ncia de estudos que interpretem as
caracter√≠sticas lingu√≠sticas regionais se sobressai, ainda mais, em um
pa√≠s de tamanha dimens√£o como o Brasil.






{{% callout note %}}

**Please, cite this work:**

Almeida, Lucas Mussoi; Perlin, Marcelo; Reichert, Marcos Henrique (2021), "Tokenization e stemming em l√≠ngua portuguesa de discursos presidenciais published at the "Open Code Community"", Mendeley Data, V1, doi: 10.17632/k5myphbwwj.1

{{% /callout %}}
           
           
